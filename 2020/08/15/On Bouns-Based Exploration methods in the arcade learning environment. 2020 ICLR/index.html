
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jthonlab">
    <title>On Bouns-Based Exploration methods in the arcade learning environment. 2020 ICLR - Jthonlab</title>
    <meta name="author" content="Jthon lab">
    
        <meta name="keywords" content="Reinforcement Learning,Exploration">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg"},"articleBody":"这是一篇针对强化学习领域exploration的一篇综述性文章，这篇文章主要将近些年的基于bouns的exploration的方法进行了一个综合比较，其中包括了pesudo-count、ICM、RND、NoisyNet与最基础的e-greedy等exploration的机制。通过实验发现其中某些算法虽然在Hard Exploration Games如Montezuma’s Revenge中表现很优秀，但是整体的性能并没有比e-greedy高出太多，说明在exploration方面仍然有许多值得研究的问题。这篇文章发表于ICLR 2020。\n\nExploration AlgorithmBouns-based exploration的方法是希望添加intrinsic reward来提升agent的探索效率。第一种方法是通过衡量出不同状态的访问次数来定义novelty。另外一种方法是通过衡量下一个状态的suprising的程度来定义novelty。\n1. Pesudo-CountPesudo-Count是针对Count-based exploration在高维状态空间的一种拓展。在Tabular的状态空间中，我们很容易能够统计出每个状态的访问次数，因此根据访问次数很容易定义出novelty,因此2008年等人在Tabular Case中提出使用这样的优化目标来提升强化学习的探索效率。但是这样的方法是不能直接放在连续状态空间中使用的，因为几乎每一个状态的访问次数都是0。为了解决这个问题，那么就会想如何定义出一种针对连续状态空间的count。有的团队从数据降维的角度来入手解决这一问题，如et.al提出使用一个autoencoder模型，将中间的embedding做成一种binary的形式来将高维图像进行降维，从而通过对embedding进行计数来实现count-based exploration。另外有一些人从density model入手，Bellemare等人在2016年使用CTS密度模型，而后续et.al使用PixelCNN的方式来建模，通过对density model的改进实现了性能的提升。在Pesudo-Count的设定当中，是假设算法有一个模型能够进行概率密度的估计，然后通过概率密度的变化来得到Pesudo-count。具体来说：一个状态的密度应该等于出现的次数除以总次数，即\n $ \\rho(x)= \\frac{N(x)}{n} $ \n其中N(x)代表pesudo-count,是未知的，而总次数n也是未知的，因此需要第二个方程。那么就可以假设当下一时刻再次遇到同一状态时，密度应该变为\n $ \\rho^'(x)= \\frac{N(x)+1}{n+1} $ \n将两个方程联立可以得到$N(X)$的解为:\n $ N(x) = \\frac{\\rho(x)(1-\\rho^'(x))}{\\rho^'(x)-\\rho(x)} \n具体这个方程的含义就是，当遇到了一个新状态x之后，势必会带来密度模型的变化，然后根据前后两个密度模型的差距可以估计得到这个新状态x的pesudo-count用来帮助exploration。\n\n2. Intrinsic Curiosity ModuleICM这个方法是通过衡量prediction error来定义novelty。具体来说，当根据状态$s_t$以及动作$a_t$来进行$s_{t+1}$的预测时，预测的越准说明这样的${(s,a)}$pairs经历的越多，那么novelty也就越低，反之则更高。但是在强化学习的场景中，并不是所有的环境元素都能够通过自己的$a_t$来影响，环境中会包括一些随机元素，这些应该与novelty是无关的。因此ICM提出了学一个Inverse Dynamic Module, 通过Siamese Network获得针对$s_t$和$s_{t+1}$的embedding，通过这两个embedding对$a_t$进行预测。这样学习出的特征表达能够过滤掉与自身动作无关的元素。接下来再建立一个dynamic model，这个模型通过使用上述Siamese Network提取出的$s_t$的特征表达与对$a_t$的特征表达进行结合，对$s_{t+1}$的特征表达进行预测，预测误差作为intrinsic reward。这个方法的具体流程如下图所示。 \n3. Random Network DistillationRND是我认为的一个非常巧妙的方法，ICM的整体流程还是较为复杂的，而RND看起来就简单的多。首先整个问题仍然是如何定义一个metric来衡量状态的novelty，那么需要达到的要求是：某一个状态遇到的越多，这个metric数值越低，某一个状态遇到的越少，这个metric数值越高。这好像与训练神经网络的方式非常相似，训练的次数越多，loss越低，训练的次数越少，loss越高。因此这篇文章就用了一个随机参数的神经网络来作为“label”,训练另外一个结构相同的网络去模仿这个随机参数的神经网络。所以说，当某一个状态训练的次数越多的时候，得到的loss也会越小，那么把这个loss作为一种intrinsic reward也能够用来衡量novelty。\n4. Noisy NetworksNoisy Networks这个探索方式比较简单，像是DDPG等强化学习算法，为了实现exploration与exploitation之间的平衡，会在网络的输出上加入一个噪声，来确保policy不会陷入到局部最优。Noisy Networks不是在输出上面加入噪声，而是在网络参数上加入噪声，从而实现exploration。\nExperimental Protocol由于上述的几种方法的论文中采用了不同的强化学习算法，不同数量的training frames以及不同的训练方式等等，因为这些差异，很难更加科学的评判出究竟哪种算法更优秀，因此本文统一了一个训练方式：强化学习算法统一为Rainbow DQN,training frames固定为200 million,统一在环境中加入sticky action(0.25)。\nPerformance on Hard Exploration Games首先，作者评估了不同算法在Montezuma’s Revenge上的表现，并通过这个游戏的表现来调节出一套较为优秀的Hyperparameters。不同算法在这款游戏上的表现如图所示。从图中能够看出，确实在这个游戏中，所有上述算法都会超过e-greedy很多，证明了这类方法的有效性。\n \n之后作者又使用同样的Hyperparameters在其它几个Hard Games上进行了测试，发现不同算法呈现出的效果并不像在Montezuma’s Revenge上面那么好，结果如下图所示。\n \nPerformance on Easy Games作者又把这些exploration的方法放到简单的游戏上进行测试，发现很多算法反而在这些游戏上带来了副作用，只有noisynet在大多数游戏上都能胜过e-greedy,其它算法甚至还不如e-greedy，作者分析这类的exploration机制可能overfitting在Montezuma’s Revenge这款游戏上，其实在其它的游戏中，这样的exploration bouns并不能提升实际表现。在简单游戏上的表现如下图所示。\n \nPerformance on whole Atari在整个Atari游戏平台上，不同的算法表现如下图所示，可以发现大部分的Exploration bonus虽然在Montezuma’s Revenge上的表现得到了大幅提升，但是在很多游戏上都带来了副作用，整体来说NoisyNet的表现还是比较优秀的。\n \nInfluence of Hyperparameter但是由于之前实验的超参数是根据Montezuma’s Revenge游戏上的表现来进行调节的，会不会是因为超参数的设置导致这些算法表现不佳的，因此作者更换了数据集对Hyperparameters进行了调节，并重新对CTS和RND两种算法进行测试，之所以选取这两个算法是因为这两个算法对Hyperparameters比较敏感，在更新Hyperparameters的表现如下图所示，更新后的表现如下图所示。对比两幅图能够发现，虽然更新了超参数之后能够提升算法的整体表现，但整体提升的性能有限，因此说明bouns-based的这几种exploration方法并不能够泛化到较为广泛的场景。 \nInfluence of training frames除此之外作者另外做了一个实验来探究training frames的数量是否对算法性能表现有明显的影响。因此作者将使用200 million frames训练得到的算法与使用1 billion训练得到的算法性能进行比较，使用1 billion frames的结果如下图所示，可以跟之前的图进行对比看出，training frames的数量并不会带来明显的影响。 \nConclusion从这些实验结果表明，很多bouns-based exploration算法实际带来的作用并不大，有一些算法在论文中所提出的效果提升可能不只是bouns带来的效果，有可能是训练过程中其它模块所带来的作用，因此在exploration这方面仍然还有很多路要走。\n","dateCreated":"2020-08-15T15:55:20+08:00","dateModified":"2020-09-05T19:40:43+08:00","datePublished":"2020-08-15T15:55:20+08:00","description":"这是一篇针对强化学习领域exploration的一篇综述性文章，这篇文章主要将近些年的基于bouns的exploration的方法进行了一个综合比较，其中包括了pesudo-count、ICM、RND、NoisyNet与最基础的e-greedy等exploration的机制。通过实验发现其中某些算法虽然在Hard Exploration Games如Montezuma’s Revenge中表现很优秀，但是整体的性能并没有比e-greedy高出太多，说明在exploration方面仍然有许多值得研究的问题。这篇文章发表于ICLR 2020。","headline":"On Bouns-Based Exploration methods in the arcade learning environment. 2020 ICLR","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2020/08/15/On%20Bouns-Based%20Exploration%20methods%20in%20the%20arcade%20learning%20environment.%202020%20ICLR/"},"publisher":{"@type":"Organization","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg","logo":{"@type":"ImageObject","url":"avatar_duck.jpg"}},"url":"http://yoursite.com/2020/08/15/On%20Bouns-Based%20Exploration%20methods%20in%20the%20arcade%20learning%20environment.%202020%20ICLR/"}</script>
    <meta name="description" content="这是一篇针对强化学习领域exploration的一篇综述性文章，这篇文章主要将近些年的基于bouns的exploration的方法进行了一个综合比较，其中包括了pesudo-count、ICM、RND、NoisyNet与最基础的e-greedy等exploration的机制。通过实验发现其中某些算法虽然在Hard Exploration Games如Montezuma’s Revenge中表现很优">
<meta property="og:type" content="blog">
<meta property="og:title" content="On Bouns-Based Exploration methods in the arcade learning environment. 2020 ICLR">
<meta property="og:url" content="http://yoursite.com/2020/08/15/On%20Bouns-Based%20Exploration%20methods%20in%20the%20arcade%20learning%20environment.%202020%20ICLR/index.html">
<meta property="og:site_name" content="Jthonlab">
<meta property="og:description" content="这是一篇针对强化学习领域exploration的一篇综述性文章，这篇文章主要将近些年的基于bouns的exploration的方法进行了一个综合比较，其中包括了pesudo-count、ICM、RND、NoisyNet与最基础的e-greedy等exploration的机制。通过实验发现其中某些算法虽然在Hard Exploration Games如Montezuma’s Revenge中表现很优">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/ICM.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Montezuma1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Hardgames1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Easygames1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-whole.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-origin.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-tune.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/large-amount.png">
<meta property="article:published_time" content="2020-08-15T07:55:20.344Z">
<meta property="article:modified_time" content="2020-09-05T11:40:43.382Z">
<meta property="article:author" content="Jthon lab">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/ICM.png">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com/assets/images/avatar_duck.jpg"/>
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-ahqseqdv7x8dyzqwhdknvcrrww0emg3sjq868izfcneq33qr7ode8shlnqqr.min.css">

    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/%20"
            aria-label=""
        >
            Jthonlab
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jthon lab</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/Jthon-lab?tab=repositories"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            On Bouns-Based Exploration methods in the arcade learning environment. 2020 ICLR
        </h1>
    
    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>这是一篇针对强化学习领域exploration的一篇综述性文章，这篇文章主要将近些年的基于bouns的exploration的方法进行了一个综合比较，其中包括了pesudo-count、ICM、RND、NoisyNet与最基础的e-greedy等exploration的机制。通过实验发现其中某些算法虽然在Hard Exploration Games如Montezuma’s Revenge中表现很优秀，但是整体的性能并没有比e-greedy高出太多，说明在exploration方面仍然有许多值得研究的问题。这篇文章发表于ICLR 2020。</p>
<a id="more"></a>
<h2 id="Exploration-Algorithm"><a href="#Exploration-Algorithm" class="headerlink" title="Exploration Algorithm"></a>Exploration Algorithm</h2><p>Bouns-based exploration的方法是希望添加intrinsic reward来提升agent的探索效率。第一种方法是通过衡量出不同状态的访问次数来定义novelty。另外一种方法是通过衡量下一个状态的suprising的程度来定义novelty。</p>
<h3 id="1-Pesudo-Count"><a href="#1-Pesudo-Count" class="headerlink" title="1. Pesudo-Count"></a>1. Pesudo-Count</h3><p>Pesudo-Count是针对Count-based exploration在高维状态空间的一种拓展。在Tabular的状态空间中，我们很容易能够统计出每个状态的访问次数，因此根据访问次数很容易定义出novelty,因此2008年等人在Tabular Case中提出使用这样的优化目标来提升强化学习的探索效率。但是这样的方法是不能直接放在连续状态空间中使用的，因为几乎每一个状态的访问次数都是0。为了解决这个问题，那么就会想如何定义出一种针对连续状态空间的count。有的团队从数据降维的角度来入手解决这一问题，如et.al提出使用一个autoencoder模型，将中间的embedding做成一种binary的形式来将高维图像进行降维，从而通过对embedding进行计数来实现count-based exploration。另外有一些人从density model入手，Bellemare等人在2016年使用CTS密度模型，而后续et.al使用PixelCNN的方式来建模，通过对density model的改进实现了性能的提升。在Pesudo-Count的设定当中，是假设算法有一个模型能够进行概率密度的估计，然后通过概率密度的变化来得到Pesudo-count。具体来说：一个状态的密度应该等于出现的次数除以总次数，即</p>
<center> $ \rho(x)= \frac{N(x)}{n} $ </center>
其中N(x)代表pesudo-count,是未知的，而总次数n也是未知的，因此需要第二个方程。那么就可以假设当下一时刻再次遇到同一状态时，密度应该变为
<center> $ \rho^'(x)= \frac{N(x)+1}{n+1} $ </center>
将两个方程联立可以得到$N(X)$的解为:
<center> $ N(x) = \frac{\rho(x)(1-\rho^'(x))}{\rho^'(x)-\rho(x)} </center>
具体这个方程的含义就是，当遇到了一个新状态x之后，势必会带来密度模型的变化，然后根据前后两个密度模型的差距可以估计得到这个新状态x的pesudo-count用来帮助exploration。

<h3 id="2-Intrinsic-Curiosity-Module"><a href="#2-Intrinsic-Curiosity-Module" class="headerlink" title="2. Intrinsic Curiosity Module"></a>2. Intrinsic Curiosity Module</h3><p>ICM这个方法是通过衡量prediction error来定义novelty。具体来说，当根据状态$s_t$以及动作$a_t$来进行$s_{t+1}$的预测时，预测的越准说明这样的${(s,a)}$pairs经历的越多，那么novelty也就越低，反之则更高。但是在强化学习的场景中，并不是所有的环境元素都能够通过自己的$a_t$来影响，环境中会包括一些随机元素，这些应该与novelty是无关的。因此ICM提出了学一个Inverse Dynamic Module, 通过Siamese Network获得针对$s_t$和$s_{t+1}$的embedding，通过这两个embedding对$a_t$进行预测。这样学习出的特征表达能够过滤掉与自身动作无关的元素。接下来再建立一个dynamic model，这个模型通过使用上述Siamese Network提取出的$s_t$的特征表达与对$a_t$的特征表达进行结合，对$s_{t+1}$的特征表达进行预测，预测误差作为intrinsic reward。这个方法的具体流程如下图所示。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/ICM.png" alt="ICM"> </p>
<h3 id="3-Random-Network-Distillation"><a href="#3-Random-Network-Distillation" class="headerlink" title="3. Random Network Distillation"></a>3. Random Network Distillation</h3><p>RND是我认为的一个非常巧妙的方法，ICM的整体流程还是较为复杂的，而RND看起来就简单的多。首先整个问题仍然是如何定义一个metric来衡量状态的novelty，那么需要达到的要求是：某一个状态遇到的越多，这个metric数值越低，某一个状态遇到的越少，这个metric数值越高。这好像与训练神经网络的方式非常相似，训练的次数越多，loss越低，训练的次数越少，loss越高。因此这篇文章就用了一个随机参数的神经网络来作为“label”,训练另外一个结构相同的网络去模仿这个随机参数的神经网络。所以说，当某一个状态训练的次数越多的时候，得到的loss也会越小，那么把这个loss作为一种intrinsic reward也能够用来衡量novelty。</p>
<h3 id="4-Noisy-Networks"><a href="#4-Noisy-Networks" class="headerlink" title="4. Noisy Networks"></a>4. Noisy Networks</h3><p>Noisy Networks这个探索方式比较简单，像是DDPG等强化学习算法，为了实现exploration与exploitation之间的平衡，会在网络的输出上加入一个噪声，来确保policy不会陷入到局部最优。Noisy Networks不是在输出上面加入噪声，而是在网络参数上加入噪声，从而实现exploration。</p>
<h2 id="Experimental-Protocol"><a href="#Experimental-Protocol" class="headerlink" title="Experimental Protocol"></a>Experimental Protocol</h2><p>由于上述的几种方法的论文中采用了不同的强化学习算法，不同数量的training frames以及不同的训练方式等等，因为这些差异，很难更加科学的评判出究竟哪种算法更优秀，因此本文统一了一个训练方式：强化学习算法统一为Rainbow DQN,training frames固定为200 million,统一在环境中加入sticky action(0.25)。</p>
<h2 id="Performance-on-Hard-Exploration-Games"><a href="#Performance-on-Hard-Exploration-Games" class="headerlink" title="Performance on Hard Exploration Games"></a>Performance on Hard Exploration Games</h2><p>首先，作者评估了不同算法在Montezuma’s Revenge上的表现，并通过这个游戏的表现来调节出一套较为优秀的Hyperparameters。不同算法在这款游戏上的表现如图所示。从图中能够看出，确实在这个游戏中，所有上述算法都会超过e-greedy很多，证明了这类方法的有效性。</p>
<p><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Montezuma1.png" alt="Montezuma1"> </p>
<p>之后作者又使用同样的Hyperparameters在其它几个Hard Games上进行了测试，发现不同算法呈现出的效果并不像在Montezuma’s Revenge上面那么好，结果如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Hardgames1.png" alt="Hardgame1"> </p>
<h2 id="Performance-on-Easy-Games"><a href="#Performance-on-Easy-Games" class="headerlink" title="Performance on Easy Games"></a>Performance on Easy Games</h2><p>作者又把这些exploration的方法放到简单的游戏上进行测试，发现很多算法反而在这些游戏上带来了副作用，只有noisynet在大多数游戏上都能胜过e-greedy,其它算法甚至还不如e-greedy，作者分析这类的exploration机制可能overfitting在Montezuma’s Revenge这款游戏上，其实在其它的游戏中，这样的exploration bouns并不能提升实际表现。在简单游戏上的表现如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Easygames1.png" alt="Easygame1"> </p>
<h2 id="Performance-on-whole-Atari"><a href="#Performance-on-whole-Atari" class="headerlink" title="Performance on whole Atari"></a>Performance on whole Atari</h2><p>在整个Atari游戏平台上，不同的算法表现如下图所示，可以发现大部分的Exploration bonus虽然在Montezuma’s Revenge上的表现得到了大幅提升，但是在很多游戏上都带来了副作用，整体来说NoisyNet的表现还是比较优秀的。</p>
<p><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-whole.png" alt="Easygame1"> </p>
<h2 id="Influence-of-Hyperparameter"><a href="#Influence-of-Hyperparameter" class="headerlink" title="Influence of Hyperparameter"></a>Influence of Hyperparameter</h2><p>但是由于之前实验的超参数是根据Montezuma’s Revenge游戏上的表现来进行调节的，会不会是因为超参数的设置导致这些算法表现不佳的，因此作者更换了数据集对Hyperparameters进行了调节，并重新对CTS和RND两种算法进行测试，之所以选取这两个算法是因为这两个算法对Hyperparameters比较敏感，在更新Hyperparameters的表现如下图所示，更新后的表现如下图所示。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-origin.png" alt="hyper1"><br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/Atari-tune.png" alt="hyper-tune"><br>对比两幅图能够发现，虽然更新了超参数之后能够提升算法的整体表现，但整体提升的性能有限，因此说明bouns-based的这几种exploration方法并不能够泛化到较为广泛的场景。 </p>
<h2 id="Influence-of-training-frames"><a href="#Influence-of-training-frames" class="headerlink" title="Influence of training frames"></a>Influence of training frames</h2><p>除此之外作者另外做了一个实验来探究training frames的数量是否对算法性能表现有明显的影响。因此作者将使用200 million frames训练得到的算法与使用1 billion训练得到的算法性能进行比较，使用1 billion frames的结果如下图所示，可以跟之前的图进行对比看出，training frames的数量并不会带来明显的影响。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.8.15/large-amount.png" alt="hyper-tune"> </p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>从这些实验结果表明，很多bouns-based exploration算法实际带来的作用并不大，有一些算法在论文中所提出的效果提升可能不只是bouns带来的效果，有可能是训练过程中其它模块所带来的作用，因此在exploration这方面仍然还有很多路要走。</p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Jthon lab. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jthon lab</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>RL</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Nanjing,China
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-spuaps2qplesxnme8e5wnd40hjkfzjjsqbnwhi8zohmoqbez3cm8pj5m7ejx.min.js"></script>

<!--SCRIPTS END-->





    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

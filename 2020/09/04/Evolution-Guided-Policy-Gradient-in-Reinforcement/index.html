
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jthonlab">
    <title>Evolution-Guided Policy Gradient in Reinforcement Learning. 2018 NIPS 笔记 - Jthonlab</title>
    <meta name="author" content="Jthon lab">
    
        <meta name="keywords" content="Reinforcement Learning,Evolution Algorithm">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg"},"articleBody":"这篇文章的思想非常有意思，它将进化算法与深度强化学习结合了起来，发表于NIPS 2018, 这篇文章的出发点在于深度强化学习会面临如下几个挑战：(1) Temporal credit assignment with sparse rewards,即在sparse reward的场景下，评价行为的好坏变得更加困难。(2) Lack of effective exploration,如何高效探索防止陷入局部最优一直是强化学习的一个问题。(3) Brittle convergence properties that are extremely sensitive to hyperparameters,同样强化学习对超参数非常敏感而且不容易收敛。这篇文章通过将传统的优化算法EA和DRL相结合的方式，使得算法性能超出了原先的EA与DRL算法。\n\nEvolution Algorithms　　这篇文章最有意思的就是将遗传算法整合到了DRL的框架之中，而在这篇文章中，种群的定义是Actor Network，在它的代码里面种群中的个体就是一个pytorch model,而它需要进化的参数其实就是Actor Network中每一层的weight和bias。在这篇文章中，作者将问题的建模为单目标优化问题，强化学习的目标相匹配，EA希望保留那些能够获得更高cumulative rewards的个体。我大概看了一下这篇论文的源码，从源码的角度总结一下这里的遗传算法的步骤：　　在源码中，每一个epoch的输入包括population,fitness_evals,migration　　它的流程主要分为四个部分：Evaluation,Selection,Crossover,Mutation。    整个流程的Pesudocode如Figure 1所示。\nFigure 3.Pesudocode\n\n(1) Evaluation　　对于EA算法，需要有一个Fitness函数来评价不同个体的优劣性。在这篇文章中直接利用cumulative reward作为评价指标，这也是非常直接的一个想法。通过把population中每一个Actor Network与环境进行交互，从而得到他们各自的episode score。根据$\\frac{score}{length}$得到的平均收益作为fitness的评价指标。这一部分的Pesudocode如Figure 2所示。\nFigure 2.Pesudocode of Evaluation\n\n(2) Selection　　在代码中，Selection这个阶段比较复杂，首先它先从population中选出e个fitness最高的actor作为Elite, 接下来population中所有成员通过tournament的方式产生另外再产生n个后代，tournament的规则也很简单：总共进行n次比赛，每一次随机抽出m个成员进行比赛，分数最高者胜出，然后添加到后代中。至此一共产生了e个Elite和n个后代，因此可以找到在population中那些没有被选中的成员。接下来把这些没有被选中的成员由DRL的policy替换掉。最终确保使得population中的成员来源于三个途径：fitness较高的Elite成员，通过tournamet胜出的成员，还有被DRL替换掉的成员。\n(3) Crossover　　在Crossover这个阶段，输入是两个Actor Network,我们将其记为$\\pi_a$和$\\pi_b$,这个阶段的目的是将$\\pi_a$和$\\pi_b$进行参数空间上的crossover,在代码中作者将其分成了两种情况:第一种是对权重$W_a$进行crossover,一般来说$W_a$往往是一个二维矩阵，因此维度是非常高的，所以作者没有使用单个元素的crossover，而是以每一行元素作为基本单位来进行crossover，具体来说，就是有50%的概率，将$\\pi_a$的权重$W_a$的第i行替换为$W_b$的第i行，50%的概率将$\\pi_b$的权重$W_b$的第i行替换为$W_a$的第i行。\n(4) Mutation　　在Mutation这个阶段，首先需要设置mutation-probablity, 对于Actor Network中的参数进行遍历时，根据mutation probablity对于某一些权重进mutation操作。而mutation是可以发生在weight和bias中的每个元素的。算法中具体也用了三种mutation方式，第一种是super mutation,也就是对于原始权重元素$W_{ij}$或者是$b_{ij}$可以进行一个幅度较大的改变，第二种是reset mutation,也就是将$W_{ij}$或者是$b_{ij}$重新进行初始化,第三种是normal mutation,可以对$W_{ij}$或者是$b_{ij}$进行幅度较小的改变。通过mutation的方式，增加了搜索的diversity。Mutation的Pesudocode如Figure 3所示。\nFigure 3.Pesudocode of Mutation\n\nDeep Reinforcement Learning　　在论文中，作者使用的算法是DDPG, 但是在代码中把它更新为了SAC，这一部分并没有什么特殊的技巧，任何一个off-policy的算法应该都可以符合这篇文章的要求,具体技术细节可以参考DDPG以及SAC的原文。\nCombined DDPG with Evolution Algorithm　　这篇文章的重点就在于成功的将DRL与EA结合在了一起。具体是怎么结合的呢？　　首先，强化学习算法选用一个off-policy的算法，例如DDPG，SAC等算法，这些算法会对应一个Actor-Critic架构，记作$Actor_{rl}$和$Critic_{rl}$,这一部分的网络参数是根据一个ReplayBuffer中采集的Transition来更新的。那么EA是怎么帮助提升强化学习算法的学习效率的呢？EA中population里面的每一个Actor Network都能够与环境进行交互而采集样本，而理想状态下这些不同Policy的分布应该是比较diverse的，同时又能够获得较高的cumulative rewards。因此可以把这些Actor Network采集到的样本添加到ReplayBuffer中去，与简单的强化学习算法相比，这样的方式提升了exploration的效率，因为这样能够保证Policy的diversity。但是从第一部分介绍的EA算法也能看出，由于population中的每一个成员都代表一个Actor Network,所以待搜索的空间是非常大的，单纯进行EA方式的探索可能会导致EA需要迭代非常多的次数才能够进化出表现较好的Actor Network，这样的话Actor Network采集到的样本学习价值就没有那么高。因此作者将DDPG(SAC)的Actor网络也添加到EA的population中，统一进行迭代，这样就能够保证，当DDPG或者是SAC的算法的性能提升时，对应EA的population中也能够拥有一些Elite成员进入到种群的进化过程中，从而提升了EA的搜索效率。整体结构如Figure 4所示。\n\nFigure 4.Architecture of ERL\n\nExperimentsStandard Inverted Double Pendulum　　首先作者将自己的算法ERL与EA算法和DDPG在OpenAI gym中的Inverted Double Pendulum进行测试,这个任务中状态空间是连续的,维度为11,动作空间也是连续的,维度是1。在这个环境中,每一步都会根据倒立摆的状态而得到一定的reward,因此不存在sparse reward的问题,在这个环境中,3个算法的表现如Figure 5所示,可以看出DDPG非常快速的学习到了较为不错的控制策略,EA虽然搜索较慢但仍然能够在这个任务上搜索出一个不错的策略,ERL的性能介于二者之间。\n\nFigure 5.Performance on Standard Inverted Double Pendulum\n\nHard Inverted Double Pendulum　　为了验证ERL的有效性,作者在Inverted Double Pendulum上提升了任务的难度,作者把它变为了一个Sparse Reward的环境,在每一步获得的奖励都设置为0,只有当一个episode结束之后,才会将整个episode的cumulative reward作为最后一步的reward反馈给算法。在Hard Inverted Double Pendulum问题上DDPG,EA,ERL的表现如Figure 6所示。能够看出变成了Sparse Reward的设定之后,DDPG完全学习不出一个较好的策略,得分几乎一直是0,而EA似乎没有受到太大的影响,表现基本与Standard Inverted Double Pendulum任务上的表现持平。而ERL以较快的速度就能够学习出一个不错的控制策略,这也验证了算法的有效性。\n\nFigure 6.Performance on Hard Inverted Double Pendulum\n\nAblation of Selection Operator　　作者做了一个消融实验，因为在EA中非常重要的一个部分是Selection Operator,作者把这部分去掉,然后将ERL分别在Swimmer和HalfCheetah两个任务上进行了测试。实验结果如Figure 7所示。能够看出去掉了Selection Operator之后,ERL的表现变得非常差,作者也对此给出了解释: Selection Operator的作用在于每次根据cumulative rewards选择表现更好的个体进行保留,因此这样会导致state distribution会往那些最终episode return更高的区域进行偏移,这就类似与一种implicit prioritization, 而且这种方式与TD error不同,通过episode returns的衡量,这种prioritization的方式更适用于long-horizon task和sparse reward的场景。进而使用这样的state distribution能够帮助算法往获得higher episode returns的方向进行学习。\n\nFigure 7.Ablation of Selection Operator\n\nMujoco　　作者又将ERL与PPO,DDPG和EA在MuJoCo的6个任务上(HalfCheetah,Swimmer,Reacher,Ant,Hopper,Walker2D)进行了测试,实验结果如Figure 7所示。可以看出ERL在这6个任务上的表现基本都超过了DDPG与PPO还有EA这三个baselines。\n\nFigure 7.Performance on MuJoCo tasks\n\nInteraction between RL and EA　　作者又测试了在每次EA迭代的过程中,$Actor_{rl}$被选为Elite或是被Selected又或者是被discarded的概率,通过这样的测试能够验证EA是不是与DRL产生了有效的结合。测试结果如Figure 8所示。从图中可以看出不同任务中EA和DRL联系的紧密程度是不一样的。在Half-cheetah任务中,$Actor_{rl}$被选择为Elite的概率高达83.8%,而在swimmer中被丢弃的概率高达76%。然后作者针对Half-cheetah任务中EA的重要性做了一些解释:作者认为在Policy-Gradient的方法中,很有可能在某些交互中产生表现特别差的trajectory,然后这会导致算法的性能有可能会下降很严重,进而很难恢复到之前表现较好的参数上去,而加入了EA之后,EA能够保留一些之前表现较好的Actor Network,那么通过EA中这些Actor Network采集的样本就能够帮助DRL获得较好的gradient信息,从而较快恢复到先前表现较好的参数,然后继续通过Gradient来提升DRL的表现。但是作者好像并没有对Swimmer中DRL较大概率被Discarded的原因进行解释。\n\nFigure 8.Selection rate for synchronized $Actor_{rl}$\n\nSummary　　这篇文章通过将强化学习算法与遗传算法进行结合，带来了性能的提升，从我的角度看来这样的方式能够带来性能提升的原因主要有以下两点：(1) 通过EA的方式,增加了Policy的多样性,因此提升了Exploration的效率。(2) 由于强化学习中,Policy的更新和state distribution是高度耦合的关系,但是随着EA的加入,其实在一定程度上限制了state distribution的偏移程度,因此能够提升训练的稳定性。　　同样我非常好奇的一点在于,在这个算法中,EA的每个个体定义为一个Actor Network,其实参数量是非常大的,而EA的每次迭代过程中随机性也是相当高的,并不能够保证性能的提升,那么假如按照和这篇论文类似的方式,去掉crossover和mutation的操作,直接在Network中加入噪声,是不是也能带来类似的效果呢? \n","dateCreated":"2020-09-04T23:02:41+08:00","dateModified":"2020-09-05T19:50:55+08:00","datePublished":"2020-09-04T23:02:41+08:00","description":"这篇文章的思想非常有意思，它将进化算法与深度强化学习结合了起来，发表于NIPS 2018, 这篇文章的出发点在于深度强化学习会面临如下几个挑战：(1) Temporal credit assignment with sparse rewards,即在sparse reward的场景下，评价行为的好坏变得更加困难。(2) Lack of effective exploration,如何高效探索防止陷入局部最优一直是强化学习的一个问题。(3) Brittle convergence properties that are extremely sensitive to hyperparameters,同样强化学习对超参数非常敏感而且不容易收敛。这篇文章通过将传统的优化算法EA和DRL相结合的方式，使得算法性能超出了原先的EA与DRL算法。","headline":"Evolution-Guided Policy Gradient in Reinforcement Learning. 2018 NIPS 笔记","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2020/09/04/Evolution-Guided-Policy-Gradient-in-Reinforcement/"},"publisher":{"@type":"Organization","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg","logo":{"@type":"ImageObject","url":"avatar_duck.jpg"}},"url":"http://yoursite.com/2020/09/04/Evolution-Guided-Policy-Gradient-in-Reinforcement/"}</script>
    <meta name="description" content="这篇文章的思想非常有意思，它将进化算法与深度强化学习结合了起来，发表于NIPS 2018, 这篇文章的出发点在于深度强化学习会面临如下几个挑战：(1) Temporal credit assignment with sparse rewards,即在sparse reward的场景下，评价行为的好坏变得更加困难。(2) Lack of effective exploration,如何高效探索防止陷">
<meta property="og:type" content="blog">
<meta property="og:title" content="Evolution-Guided Policy Gradient in Reinforcement Learning. 2018 NIPS 笔记">
<meta property="og:url" content="http://yoursite.com/2020/09/04/Evolution-Guided-Policy-Gradient-in-Reinforcement/index.html">
<meta property="og:site_name" content="Jthonlab">
<meta property="og:description" content="这篇文章的思想非常有意思，它将进化算法与深度强化学习结合了起来，发表于NIPS 2018, 这篇文章的出发点在于深度强化学习会面临如下几个挑战：(1) Temporal credit assignment with sparse rewards,即在sparse reward的场景下，评价行为的好坏变得更加困难。(2) Lack of effective exploration,如何高效探索防止陷">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Pesudo-code.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Evaluate.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mutation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/ERL_Archi.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Easy-Pendulum.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Hard-Pendulum.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/abliation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mujuco.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mujuco.png">
<meta property="article:published_time" content="2020-09-04T15:02:41.000Z">
<meta property="article:modified_time" content="2020-09-05T11:50:55.026Z">
<meta property="article:author" content="Jthon lab">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Pesudo-code.png">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com/assets/images/avatar_duck.jpg"/>
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-ahqseqdv7x8dyzqwhdknvcrrww0emg3sjq868izfcneq33qr7ode8shlnqqr.min.css">

    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/%20"
            aria-label=""
        >
            Jthonlab
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jthon lab</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/Jthon-lab?tab=repositories"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Evolution-Guided Policy Gradient in Reinforcement Learning. 2018 NIPS 笔记
        </h1>
    
    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>这篇文章的思想非常有意思，它将进化算法与深度强化学习结合了起来，发表于NIPS 2018, 这篇文章的出发点在于深度强化学习会面临如下几个挑战：<br>(1) <strong>Temporal credit assignment with sparse rewards</strong>,即在sparse reward的场景下，评价行为的好坏变得更加困难。<br>(2) <strong>Lack of effective exploration</strong>,如何高效探索防止陷入局部最优一直是强化学习的一个问题。<br>(3) <strong>Brittle convergence properties that are extremely sensitive to hyperparameters</strong>,同样强化学习对超参数非常敏感而且不容易收敛。<br>这篇文章通过将传统的优化算法EA和DRL相结合的方式，使得算法性能超出了原先的EA与DRL算法。</p>
<a id="more"></a>
<h2 id="Evolution-Algorithms"><a href="#Evolution-Algorithms" class="headerlink" title="Evolution Algorithms"></a>Evolution Algorithms</h2><p>　　这篇文章最有意思的就是将遗传算法整合到了DRL的框架之中，而在这篇文章中，种群的定义是Actor Network，在它的代码里面种群中的个体就是一个pytorch model,而它需要进化的参数其实就是Actor Network中每一层的weight和bias。在这篇文章中，作者将问题的建模为单目标优化问题，强化学习的目标相匹配，EA希望保留那些能够获得更高cumulative rewards的个体。我大概看了一下这篇论文的源码，从源码的角度总结一下这里的遗传算法的步骤：<br>　　在源码中，每一个epoch的输入包括population,fitness_evals,migration<br>　　它的流程主要分为四个部分：<strong>Evaluation,Selection,Crossover,Mutation</strong>。<br>    整个流程的Pesudocode如Figure 1所示。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Pesudo-code.png" alt="Pesudocode"></p>
<center><b>Figure 3.Pesudocode</b></center>

<h4 id="1-Evaluation"><a href="#1-Evaluation" class="headerlink" title="(1) Evaluation"></a>(1) Evaluation</h4><p>　　对于EA算法，需要有一个Fitness函数来评价不同个体的优劣性。在这篇文章中直接利用cumulative reward作为评价指标，这也是非常直接的一个想法。通过把population中每一个Actor Network与环境进行交互，从而得到他们各自的episode score。根据$\frac{score}{length}$得到的平均收益作为fitness的评价指标。这一部分的Pesudocode如Figure 2所示。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Evaluate.png" alt="Evaluation"></p>
<center><b>Figure 2.Pesudocode of Evaluation</b></center>

<h4 id="2-Selection"><a href="#2-Selection" class="headerlink" title="(2) Selection"></a>(2) Selection</h4><p>　　在代码中，Selection这个阶段比较复杂，首先它先从population中选出e个fitness最高的actor作为Elite, 接下来population中所有成员通过tournament的方式产生另外再产生n个后代，tournament的规则也很简单：总共进行n次比赛，每一次随机抽出m个成员进行比赛，分数最高者胜出，然后添加到后代中。至此一共产生了e个Elite和n个后代，因此可以找到在population中那些没有被选中的成员。接下来把这些没有被选中的成员由DRL的policy替换掉。最终确保使得population中的成员来源于三个途径：fitness较高的Elite成员，通过tournamet胜出的成员，还有被DRL替换掉的成员。</p>
<h4 id="3-Crossover"><a href="#3-Crossover" class="headerlink" title="(3) Crossover"></a>(3) Crossover</h4><p>　　在Crossover这个阶段，输入是两个Actor Network,我们将其记为$\pi_a$和$\pi_b$,这个阶段的目的是将$\pi_a$和$\pi_b$进行参数空间上的crossover,在代码中作者将其分成了两种情况:第一种是对权重$W_a$进行crossover,一般来说$W_a$往往是一个二维矩阵，因此维度是非常高的，所以作者没有使用单个元素的crossover，而是以每一行元素作为基本单位来进行crossover，具体来说，就是有50%的概率，将$\pi_a$的权重$W_a$的第i行替换为$W_b$的第i行，50%的概率将$\pi_b$的权重$W_b$的第i行替换为$W_a$的第i行。</p>
<h4 id="4-Mutation"><a href="#4-Mutation" class="headerlink" title="(4) Mutation"></a>(4) Mutation</h4><p>　　在Mutation这个阶段，首先需要设置mutation-probablity, 对于Actor Network中的参数进行遍历时，根据mutation probablity对于某一些权重进mutation操作。而mutation是可以发生在weight和bias中的每个元素的。算法中具体也用了三种mutation方式，第一种是super mutation,也就是对于原始权重元素$W_{ij}$或者是$b_{ij}$可以进行一个幅度较大的改变，第二种是reset mutation,也就是将$W_{ij}$或者是$b_{ij}$重新进行初始化,第三种是normal mutation,可以对$W_{ij}$或者是$b_{ij}$进行幅度较小的改变。通过mutation的方式，增加了搜索的diversity。Mutation的Pesudocode如Figure 3所示。<br><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mutation.png" alt="Mutation"></p>
<center><b>Figure 3.Pesudocode of Mutation</b></center>

<h2 id="Deep-Reinforcement-Learning"><a href="#Deep-Reinforcement-Learning" class="headerlink" title="Deep Reinforcement Learning"></a>Deep Reinforcement Learning</h2><p>　　在论文中，作者使用的算法是DDPG, 但是在代码中把它更新为了SAC，这一部分并没有什么特殊的技巧，任何一个off-policy的算法应该都可以符合这篇文章的要求,具体技术细节可以参考DDPG以及SAC的原文。</p>
<h2 id="Combined-DDPG-with-Evolution-Algorithm"><a href="#Combined-DDPG-with-Evolution-Algorithm" class="headerlink" title="Combined DDPG with Evolution Algorithm"></a>Combined DDPG with Evolution Algorithm</h2><p>　　这篇文章的重点就在于成功的将DRL与EA结合在了一起。具体是怎么结合的呢？<br>　　首先，强化学习算法选用一个off-policy的算法，例如DDPG，SAC等算法，这些算法会对应一个Actor-Critic架构，记作$Actor_{rl}$和$Critic_{rl}$,这一部分的网络参数是根据一个ReplayBuffer中采集的Transition来更新的。那么EA是怎么帮助提升强化学习算法的学习效率的呢？EA中population里面的每一个Actor Network都能够与环境进行交互而采集样本，而理想状态下这些不同Policy的分布应该是比较diverse的，同时又能够获得较高的cumulative rewards。因此可以把这些Actor Network采集到的样本添加到ReplayBuffer中去，与简单的强化学习算法相比，这样的方式提升了exploration的效率，因为这样能够保证Policy的diversity。但是从第一部分介绍的EA算法也能看出，由于population中的每一个成员都代表一个Actor Network,所以待搜索的空间是非常大的，单纯进行EA方式的探索可能会导致EA需要迭代非常多的次数才能够进化出表现较好的Actor Network，这样的话Actor Network采集到的样本学习价值就没有那么高。因此作者将DDPG(SAC)的Actor网络也添加到EA的population中，统一进行迭代，这样就能够保证，当DDPG或者是SAC的算法的性能提升时，对应EA的population中也能够拥有一些Elite成员进入到种群的进化过程中，从而提升了EA的搜索效率。整体结构如Figure 4所示。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/ERL_Archi.png"/></div>
<center><b>Figure 4.Architecture of ERL</b></center>

<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h4 id="Standard-Inverted-Double-Pendulum"><a href="#Standard-Inverted-Double-Pendulum" class="headerlink" title="Standard Inverted Double Pendulum"></a>Standard Inverted Double Pendulum</h4><p>　　首先作者将自己的算法ERL与EA算法和DDPG在OpenAI gym中的Inverted Double Pendulum进行测试,这个任务中状态空间是连续的,维度为11,动作空间也是连续的,维度是1。在这个环境中,每一步都会根据倒立摆的状态而得到一定的reward,因此不存在sparse reward的问题,在这个环境中,3个算法的表现如Figure 5所示,可以看出DDPG非常快速的学习到了较为不错的控制策略,EA虽然搜索较慢但仍然能够在这个任务上搜索出一个不错的策略,ERL的性能介于二者之间。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Easy-Pendulum.png"/></div>
<center><b>Figure 5.Performance on Standard Inverted Double Pendulum</b></center>

<h4 id="Hard-Inverted-Double-Pendulum"><a href="#Hard-Inverted-Double-Pendulum" class="headerlink" title="Hard Inverted Double Pendulum"></a>Hard Inverted Double Pendulum</h4><p>　　为了验证ERL的有效性,作者在Inverted Double Pendulum上提升了任务的难度,作者把它变为了一个Sparse Reward的环境,在每一步获得的奖励都设置为0,只有当一个episode结束之后,才会将整个episode的cumulative reward作为最后一步的reward反馈给算法。在Hard Inverted Double Pendulum问题上DDPG,EA,ERL的表现如Figure 6所示。能够看出变成了Sparse Reward的设定之后,DDPG完全学习不出一个较好的策略,得分几乎一直是0,而EA似乎没有受到太大的影响,表现基本与Standard Inverted Double Pendulum任务上的表现持平。而ERL以较快的速度就能够学习出一个不错的控制策略,这也验证了算法的有效性。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Hard-Pendulum.png"/></div>
<center><b>Figure 6.Performance on Hard Inverted Double Pendulum</b></center>

<h3 id="Ablation-of-Selection-Operator"><a href="#Ablation-of-Selection-Operator" class="headerlink" title="Ablation of Selection Operator"></a>Ablation of Selection Operator</h3><p>　　作者做了一个消融实验，因为在EA中非常重要的一个部分是Selection Operator,作者把这部分去掉,然后将ERL分别在Swimmer和HalfCheetah两个任务上进行了测试。实验结果如Figure 7所示。能够看出去掉了Selection Operator之后,ERL的表现变得非常差,作者也对此给出了解释: Selection Operator的作用在于每次根据cumulative rewards选择表现更好的个体进行保留,因此这样会导致state distribution会往那些最终episode return更高的区域进行偏移,这就类似与一种implicit prioritization, 而且这种方式与TD error不同,通过episode returns的衡量,这种prioritization的方式更适用于long-horizon task和sparse reward的场景。进而使用这样的state distribution能够帮助算法往获得higher episode returns的方向进行学习。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/abliation.png"/></div>
<center><b>Figure 7.Ablation of Selection Operator</b></center>

<h4 id="Mujoco"><a href="#Mujoco" class="headerlink" title="Mujoco"></a>Mujoco</h4><p>　　作者又将ERL与PPO,DDPG和EA在MuJoCo的6个任务上(HalfCheetah,Swimmer,Reacher,<br>Ant,Hopper,Walker2D)进行了测试,实验结果如Figure 7所示。可以看出ERL在这6个任务上的表现基本都超过了DDPG与PPO还有EA这三个baselines。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mujuco.png"/></div>
<center><b>Figure 7.Performance on MuJoCo tasks</b></center>

<h4 id="Interaction-between-RL-and-EA"><a href="#Interaction-between-RL-and-EA" class="headerlink" title="Interaction between RL and EA"></a>Interaction between RL and EA</h4><p>　　作者又测试了在每次EA迭代的过程中,$Actor_{rl}$被选为Elite或是被Selected又或者是被discarded的概率,通过这样的测试能够验证EA是不是与DRL产生了有效的结合。测试结果如Figure 8所示。从图中可以看出不同任务中EA和DRL联系的紧密程度是不一样的。在Half-cheetah任务中,$Actor_{rl}$被选择为Elite的概率高达83.8%,而在swimmer中被丢弃的概率高达76%。然后作者针对Half-cheetah任务中EA的重要性做了一些解释:作者认为在Policy-Gradient的方法中,很有可能在某些交互中产生表现特别差的trajectory,然后这会导致算法的性能有可能会下降很严重,进而很难恢复到之前表现较好的参数上去,而加入了EA之后,EA能够保留一些之前表现较好的Actor Network,那么通过EA中这些Actor Network采集的样本就能够帮助DRL获得较好的gradient信息,从而较快恢复到先前表现较好的参数,然后继续通过Gradient来提升DRL的表现。但是作者好像并没有对Swimmer中DRL较大概率被Discarded的原因进行解释。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.9.5/Mujuco.png"/></div>
<center><b>Figure 8.Selection rate for synchronized $Actor_{rl}$</b></center>

<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>　　这篇文章通过将强化学习算法与遗传算法进行结合，带来了性能的提升，从我的角度看来这样的方式能够带来性能提升的原因主要有以下两点：<br>(1) 通过EA的方式,增加了Policy的多样性,因此提升了Exploration的效率。<br>(2) 由于强化学习中,Policy的更新和state distribution是高度耦合的关系,但是随着EA的加入,其实在一定程度上限制了state distribution的偏移程度,因此能够提升训练的稳定性。<br>　　同样我非常好奇的一点在于,在这个算法中,EA的每个个体定义为一个Actor Network,其实参数量是非常大的,而EA的每次迭代过程中随机性也是相当高的,并不能够保证性能的提升,那么假如按照和这篇论文类似的方式,去掉crossover和mutation的操作,直接在Network中加入噪声,是不是也能带来类似的效果呢? </p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Jthon lab. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jthon lab</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>RL</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Nanjing,China
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-spuaps2qplesxnme8e5wnd40hjkfzjjsqbnwhi8zohmoqbez3cm8pj5m7ejx.min.js"></script>

<!--SCRIPTS END-->





    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

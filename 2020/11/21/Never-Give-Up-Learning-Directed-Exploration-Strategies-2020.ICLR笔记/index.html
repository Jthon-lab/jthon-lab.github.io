
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jthonlab">
    <title>Never Give Up: Learning Directed Exploration Strategies.ICLR 2020 笔记 - Jthonlab</title>
    <meta name="author" content="Jthon lab">
    
        <meta name="keywords" content="Reinforcement Learning,Exploration">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg"},"articleBody":"这篇文章通过结合episodic memory以及life-long experience提出了一种新的intrinsic reward，并且通过UVFA的方式来估计值函数，通过设置权重的方式实现了trade-off between exploration and exploitation。这篇文章的方法在Atari里面hard-exploration games以及剩下的游戏中都取得了优异的表现。\n\nNever-Give-Up Intrinsic Reward与先前的基于好奇心的方式类似，这篇文章仍然希望设计出一个更合理的内在奖励来帮助探索。这篇文章首先使用Inverse Dynamic Model训练出一个embedding network，使用这个network过滤掉那些与动作无关的部分。Inverse Dynamic Model 的训练仍然能够使用自监督的方式来进行训练(参考ICM那篇论文的设计)，即通过与环境交互采集到的$(s,s_{t+1},a)$的pairs来训练。另外这篇文章比较创新的一点在于衡量了life-long的novelty以及episodic的novelty。其中life-long novelty的实现方式参考RND那篇文章的方式。episodic novelty用来鼓励一个episode中state的多样性。实现的思想是利用一个episodic memory来实现：当得到环境中的状态$s_{t+1}$时，首先使用embedding network得到特征向量$f(x_{t+1})$，根据特征向量在episodic memory中选择K个特征空间上距离较近的特征向量,记作$N_{k}=[f(x_{1}),f(x_{2}),…f(x_{k})]$，通过衡量$f(x_{t})$和$N_{k}$之间的距离来获得$r_{t}^{epi}$。具体来说：$$ r_{t}^{epi} = \\frac{1}{\\sqrt{\\sum{K(f(x_{t+1}),f(x_{i}))}+c}} $$$$ K(x,y) = \\frac{\\sigma}{\\frac{d^{2}(x,y)}{d_{m}^2}+\\sigma} $$其中d是Euclidean distance，$d_{m}^{2}$ 是一个对knn的Euclidean distance的平均值估计，$\\sigma$是一个很小的常数(实验中设置为了0.001)。接下来\n","dateCreated":"2020-11-21T19:23:48+08:00","dateModified":"2020-11-21T20:02:24+08:00","datePublished":"2020-11-21T19:23:48+08:00","description":"这篇文章通过结合episodic memory以及life-long experience提出了一种新的intrinsic reward，并且通过UVFA的方式来估计值函数，通过设置权重的方式实现了trade-off between exploration and exploitation。这篇文章的方法在Atari里面hard-exploration games以及剩下的游戏中都取得了优异的表现。","headline":"Never Give Up: Learning Directed Exploration Strategies.ICLR 2020 笔记","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2020/11/21/Never-Give-Up-Learning-Directed-Exploration-Strategies-2020.ICLR%E7%AC%94%E8%AE%B0/"},"publisher":{"@type":"Organization","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg","logo":{"@type":"ImageObject","url":"avatar_duck.jpg"}},"url":"http://yoursite.com/2020/11/21/Never-Give-Up-Learning-Directed-Exploration-Strategies-2020.ICLR%E7%AC%94%E8%AE%B0/"}</script>
    <meta name="description" content="这篇文章通过结合episodic memory以及life-long experience提出了一种新的intrinsic reward，并且通过UVFA的方式来估计值函数，通过设置权重的方式实现了trade-off between exploration and exploitation。这篇文章的方法在Atari里面hard-exploration games以及剩下的游戏中都取得了优异的表现">
<meta property="og:type" content="blog">
<meta property="og:title" content="Never Give Up: Learning Directed Exploration Strategies.ICLR 2020 笔记">
<meta property="og:url" content="http://yoursite.com/2020/11/21/Never-Give-Up-Learning-Directed-Exploration-Strategies-2020.ICLR%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Jthonlab">
<meta property="og:description" content="这篇文章通过结合episodic memory以及life-long experience提出了一种新的intrinsic reward，并且通过UVFA的方式来估计值函数，通过设置权重的方式实现了trade-off between exploration and exploitation。这篇文章的方法在Atari里面hard-exploration games以及剩下的游戏中都取得了优异的表现">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-11-21T11:23:48.000Z">
<meta property="article:modified_time" content="2020-11-21T12:02:24.624Z">
<meta property="article:author" content="Jthon lab">
<meta name="twitter:card" content="summary">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com/assets/images/avatar_duck.jpg"/>
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-ahqseqdv7x8dyzqwhdknvcrrww0emg3sjq868izfcneq33qr7ode8shlnqqr.min.css">

    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/%20"
            aria-label=""
        >
            Jthonlab
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jthon lab</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/Jthon-lab?tab=repositories"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Never Give Up: Learning Directed Exploration Strategies.ICLR 2020 笔记
        </h1>
    
    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>这篇文章通过结合episodic memory以及life-long experience提出了一种新的intrinsic reward，并且通过UVFA的方式来估计值函数，通过设置权重的方式实现了trade-off between exploration and exploitation。这篇文章的方法在Atari里面hard-exploration games以及剩下的游戏中都取得了优异的表现。</p>
<a id="more"></a>
<h2 id="Never-Give-Up-Intrinsic-Reward"><a href="#Never-Give-Up-Intrinsic-Reward" class="headerlink" title="Never-Give-Up Intrinsic Reward"></a>Never-Give-Up Intrinsic Reward</h2><p>与先前的基于好奇心的方式类似，这篇文章仍然希望设计出一个更合理的内在奖励来帮助探索。这篇文章首先使用Inverse Dynamic Model训练出一个embedding network，使用这个network过滤掉那些与动作无关的部分。Inverse Dynamic Model 的训练仍然能够使用自监督的方式来进行训练(参考ICM那篇论文的设计)，即通过与环境交互采集到的$(s,s_{t+1},a)$的pairs来训练。另外这篇文章比较创新的一点在于衡量了life-long的novelty以及episodic的novelty。其中life-long novelty的实现方式参考RND那篇文章的方式。episodic novelty用来鼓励一个episode中state的多样性。实现的思想是利用一个episodic memory来实现：当得到环境中的状态$s_{t+1}$时，首先使用embedding network得到特征向量$f(x_{t+1})$，根据特征向量在episodic memory中选择K个特征空间上距离较近的特征向量,记作$N_{k}=[f(x_{1}),f(x_{2}),…f(x_{k})]$，通过衡量$f(x_{t})$和$N_{k}$之间的距离来获得$r_{t}^{epi}$。具体来说：<br>$$ r_{t}^{epi} = \frac{1}{\sqrt{\sum{K(f(x_{t+1}),f(x_{i}))}+c}} $$<br>$$ K(x,y) = \frac{\sigma}{\frac{d^{2}(x,y)}{d_{m}^2}+\sigma} $$<br>其中d是Euclidean distance，$d_{m}^{2}$ 是一个对knn的Euclidean distance的平均值估计，$\sigma$是一个很小的常数(实验中设置为了0.001)。<br>接下来</p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Jthon lab. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jthon lab</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>RL</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Nanjing,China
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-spuaps2qplesxnme8e5wnd40hjkfzjjsqbnwhi8zohmoqbez3cm8pj5m7ejx.min.js"></script>

<!--SCRIPTS END-->





    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

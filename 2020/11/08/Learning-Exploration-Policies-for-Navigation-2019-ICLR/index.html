
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jthonlab">
    <title>Learning Exploration Policies for Navigation. 2019 ICLR 笔记 - Jthonlab</title>
    <meta name="author" content="Jthon lab">
    
        <meta name="keywords" content="Reinforcement Learning,Navigation">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg"},"articleBody":"这篇文章旨在提升新场景导航中Exploration效率问题，导航本身是一个sparse-reward的场景，如果没有一些额外的帮助搜索的机制的话会给强化学习带来困难，有一些工作使用了辅助任务来提升学习的效率，如UNREAL，还有Learning to Navigate in Complex Environments等文章。这篇文章主要的创新点在于将SLAM获取的环境地图与第一人称视角的信息相结合，提升了在新场景中的exploration效率。\n\nNavigation Problem如今导航算法主要分为了两个方面：(1) 基于建图和路径规划的方式 (2) 基于学习的方式。对于第一种方式，现在往往使用人工的方式去控制机器人来尽快建立环境地图或者使用一些启发式搜索的方式来提升探索的效率。对于第二种方式，往往在场景中定义出一个特定的任务来让智能体去完成或者是假定环境已经被探索然后再学习导航策略。但是像random policy这样简单的探索策略效率非常低，而假设环境已知则是不切实际的一个假设。虽然现在有很多工作都基于curiosity来设计一个intrinsic reward去鼓励探索行为，但是针对导航问题可能会有更高效的解决方法。这篇文章就通过将SLAM与强化学习结合的方式来提升导航的效率。\nApproach这篇文章仍然是一篇基于强化学习的文章，observation是RGB-D图像，action是离散动作包括了最基本的如前进、后退、左转、右转等。这个智能体会被放在一个novel environment,希望学到一个$\\pi_{e}$，能尽快的在这个场景中探索完全。这篇文章主要提出了如下的贡献(1) 关于$\\pi_{e}$的设计 (2) reward的设计 (3) 利用demonstrations来提升$\\pi_{e}$的训练速度。\nMap Construction这个地方使用了类似SLAM的方式来构建场景的地图，这个模块通过使用第一视角的深度图来对智能体所在的位置$\\hat{x_{t}}$ 和整个场景的俯视图进行更新，但是对位置的估计可能会出现偏差，此处没有使用非线性优化的方法来提升估计的准确性，而是希望后续的policy能够产生对于这种偏差的鲁棒性。\nPolicy Architecture为了得到一个更好的exploration的策略，这篇文章将不同分辨率的两个egocentric Map添加到网络的输入中，这样的俯视图的输入能够在一个更宏观的层面上提供有用的信息。整个算法的流程图如Figure 1所示。整个网络的特征包括俩部分:(1) 第一人称RGB图像的信息通过一个预训练好的ResNet-18来提取特征。(2) 两种不同分辨率的俯视图也通过ResNet-18来提取特征最后将这两个特征拼接起来得到当前状态的特征表达，然后通过RNN网络来形成最终的Policy。强化学习算法使用了imitation learning+PPO的方式。训练的时候首先利用Das et al.等人在House3D环境中所做的工作，直接将这个算法在环境中采集的轨迹作为专家轨迹来对网络进行训练。之后再采用PPO对算法进行优化。\n\nFigure 1. Architecture of algorithm\n\nCoverage Reward为了更有效地使用环境俯视图提取出的特征，这篇工作设计了一个coverage reward。 $C(M_{t})$记录了整个环境中所有已知经过的区域和已知的未经过的区域，那么在智能体执行过一次动作$a_{t}$后，$C(M_{t+1})$和$C(M_{t})$会有所不同，那么exploration策略的目的应该是最大化已知经过区域的变化。因此Coverage Reward记为$$ R_{cov}(t) = C(M_{t+1}) - C(M_{t}) $$同时为了让智能体拥有避障的功能，还对碰撞行为进行了惩罚。碰撞的判定是通过一个额外的bump sensor来给出的，记为$$ R_{coll}(t) = -Bump(t+1) $$整个环境的reward为:$$ r_{t} = R_{cov}+R_{coll} $$\nExperiments这篇工作使用了House3D仿真环境，针对训练和测试，分别使用了20个不同的House，共计40个House。测试使用的House和训练使用的House是完全不同的。这个场景中的observation包括了60°第一人称视角下的RGB-D相机信息和一个碰撞传感器。动作空间包括了6种基本动作，分别是：前进0.25米，后退0.25米，向左0.25米，向右0.25米，向左转9°和向右转9°。这个场景下没有任何的外部奖励，内在奖励记为$ R(t) = \\alpha R_{cov}(t)+\\beta R_{coll}(t) $。训练策略的时候，首先使用Das等人的方法采集693条human trajectories，然后使用这些轨迹进行imitation learning，形成一个最基础的policy。接下来再使用强化学习的方式形成一个更优秀的导航策略。House3D仿真环境如Figure 2所示。\n\nFigure 2. House3D screenshot\n\nBaselines这篇文章比较的Baselines主要包括两大类：(1) Froniter-based Exploration 这个方法每次都在建立好的环境地图中采样一个未被探索的目标位置，然后用路径规划的方式走到目标位置，随着这个流程不断进行，整个场景的地图可以建立的越来越完备。(2) Curiosity-based Exploration 这里作者采用了ICM类似的方式，使用prediction error作为intrinsic reward。\nWithout Estimation Noise这个场景的假设是位置估计$\\hat{x_{t}}$是完全准确的，虽然这个假设非常的奢侈，但是可以比较一下不同探索方式的优劣性。这里作者比较了如下几种探索方式：Random Policy, IL+RL with Curiosity, Straight, IL+RL with RGB, IL+RL with Map+RGB。其中Straight的探索方式是指，智能体一直保持直行，如果碰撞，则转9°之后继续直行的探索方式。IL+RL with RGB是作者本文提出的方法，但是去掉了egocentric地图的输入，保留了coverage reward和其它部分。IL+RL with Map+RGB则是作者本文方法的完整版。这个实验的结果图如Figure 3所示，可以看出作者的方法是要优于简单的Curiosity-based Exploration的。并且单纯使用RGB作为输入的方法也比较优秀。\n\nFigure 3. Exploration performance for different methods\n\nWith Estimation Noise由于上一个实验的假设比较苛刻，因此作者对位置观测加入了噪声，并比较了噪声的比例对Coverage的影响，结果如Figure 4所示。可以看出作者的方法对于噪声带来的影响要比Froniter-based方式拥有更好的鲁棒性。\n\nFigure 4. Exploration performance of different noise percentage\n\nGeometry and Affordance Mismatch作者用这个实验来说明learning的重要性。在这个实验中的设定是把House3D中的doors设置为不触发碰撞检测的障碍物，这个实验想要说明的是，当几何信息出现错误的时候，learning-based的方法拥有更强的鲁棒性，因为learning-based的方法能够从视觉上检测出门其实对应是一个障碍物从而弥补几何信息上的错误，这个实验的结果如Figure 5所示。可以看出Froniter-based方法对于几何信息的准确性较为敏感，因此在表现上有了大幅度的下降，而作者的方法并没有太多的表现下降。\n\nFigure 5. Exploration performance of mismatch for geometry\n\nAblation Study这篇文章中作者进行了三个消融实验分别验证Imitation Learning的重要性，环境地图作为输入的重要性以及coverage reward的优越性，实验结果如Figure 6所示。\n\nFigure 6. Ablation Study\n\nUsing Exploration For Downstream Task进一步验证exploration的有效性，作者利用学到的exploration策略用于goal-driven navigation任务中去。goal的采集流程是在环境中随机选择50个goal image(随机位置，随机方向)。智能体需要利用exploration策略采集的得到的经验导航至目标图片。具体做法是对于目标所在的位置，首先使用Nearest Neighbor找到experience中距离最近的一张图片，然后从环境地图中找到对应位置，接下来使用路径规划的方式到达目标图片。因此在这个实验中，exploration策略能否采集到整个环境中不同区域的信息是非常重要的。作者比较了random policy,curiosity和作者的方法，结果如Figure 7所示。\n\nFigure 7. Performance for goal-driven navigation","dateCreated":"2020-11-08T20:05:41+08:00","dateModified":"2020-11-09T11:55:53+08:00","datePublished":"2020-11-08T20:05:41+08:00","description":"这篇文章旨在提升新场景导航中Exploration效率问题，导航本身是一个sparse-reward的场景，如果没有一些额外的帮助搜索的机制的话会给强化学习带来困难，有一些工作使用了辅助任务来提升学习的效率，如UNREAL，还有Learning to Navigate in Complex Environments等文章。这篇文章主要的创新点在于将SLAM获取的环境地图与第一人称视角的信息相结合，提升了在新场景中的exploration效率。","headline":"Learning Exploration Policies for Navigation. 2019 ICLR 笔记","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2020/11/08/Learning-Exploration-Policies-for-Navigation-2019-ICLR/"},"publisher":{"@type":"Organization","name":"Jthon lab","sameAs":["https://github.com/Jthon-lab?tab=repositories"],"image":"avatar_duck.jpg","logo":{"@type":"ImageObject","url":"avatar_duck.jpg"}},"url":"http://yoursite.com/2020/11/08/Learning-Exploration-Policies-for-Navigation-2019-ICLR/"}</script>
    <meta name="description" content="这篇文章旨在提升新场景导航中Exploration效率问题，导航本身是一个sparse-reward的场景，如果没有一些额外的帮助搜索的机制的话会给强化学习带来困难，有一些工作使用了辅助任务来提升学习的效率，如UNREAL，还有Learning to Navigate in Complex Environments等文章。这篇文章主要的创新点在于将SLAM获取的环境地图与第一人称视角的信息相结合，">
<meta property="og:type" content="blog">
<meta property="og:title" content="Learning Exploration Policies for Navigation. 2019 ICLR 笔记">
<meta property="og:url" content="http://yoursite.com/2020/11/08/Learning-Exploration-Policies-for-Navigation-2019-ICLR/index.html">
<meta property="og:site_name" content="Jthonlab">
<meta property="og:description" content="这篇文章旨在提升新场景导航中Exploration效率问题，导航本身是一个sparse-reward的场景，如果没有一些额外的帮助搜索的机制的话会给强化学习带来困难，有一些工作使用了辅助任务来提升学习的效率，如UNREAL，还有Learning to Navigate in Complex Environments等文章。这篇文章主要的创新点在于将SLAM获取的环境地图与第一人称视角的信息相结合，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/architecture.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/House3D.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Methods.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Noise.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Mismatch.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Ablation-Study.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Downstreamtask.png">
<meta property="article:published_time" content="2020-11-08T12:05:41.000Z">
<meta property="article:modified_time" content="2020-11-09T03:55:53.325Z">
<meta property="article:author" content="Jthon lab">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/architecture.png">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com/assets/images/avatar_duck.jpg"/>
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-ahqseqdv7x8dyzqwhdknvcrrww0emg3sjq868izfcneq33qr7ode8shlnqqr.min.css">

    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/%20"
            aria-label=""
        >
            Jthonlab
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jthon lab</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/Jthon-lab?tab=repositories"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Learning Exploration Policies for Navigation. 2019 ICLR 笔记
        </h1>
    
    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>这篇文章旨在提升新场景导航中Exploration效率问题，导航本身是一个sparse-reward的场景，如果没有一些额外的帮助搜索的机制的话会给强化学习带来困难，有一些工作使用了辅助任务来提升学习的效率，如UNREAL，还有Learning to Navigate in Complex Environments等文章。这篇文章主要的创新点在于将SLAM获取的环境地图与第一人称视角的信息相结合，提升了在新场景中的exploration效率。</p>
<a id="more"></a>
<h2 id="Navigation-Problem"><a href="#Navigation-Problem" class="headerlink" title="Navigation Problem"></a>Navigation Problem</h2><p>如今导航算法主要分为了两个方面：(1) 基于建图和路径规划的方式 (2) 基于学习的方式。对于第一种方式，现在往往使用人工的方式去控制机器人来尽快建立环境地图或者使用一些启发式搜索的方式来提升探索的效率。对于第二种方式，往往在场景中定义出一个特定的任务来让智能体去完成或者是假定环境已经被探索然后再学习导航策略。但是像random policy这样简单的探索策略效率非常低，而假设环境已知则是不切实际的一个假设。虽然现在有很多工作都基于curiosity来设计一个intrinsic reward去鼓励探索行为，但是针对导航问题可能会有更高效的解决方法。这篇文章就通过将SLAM与强化学习结合的方式来提升导航的效率。</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>这篇文章仍然是一篇基于强化学习的文章，observation是RGB-D图像，action是离散动作包括了最基本的如前进、后退、左转、右转等。这个智能体会被放在一个novel environment,希望学到一个$\pi_{e}$，能尽快的在这个场景中探索完全。这篇文章主要提出了如下的贡献(1) 关于$\pi_{e}$的设计 (2) reward的设计 (3) 利用demonstrations来提升$\pi_{e}$的训练速度。</p>
<h4 id="Map-Construction"><a href="#Map-Construction" class="headerlink" title="Map Construction"></a>Map Construction</h4><p>这个地方使用了类似SLAM的方式来构建场景的地图，这个模块通过使用第一视角的深度图来对智能体所在的位置$\hat{x_{t}}$ 和整个场景的俯视图进行更新，但是对位置的估计可能会出现偏差，此处没有使用非线性优化的方法来提升估计的准确性，而是希望后续的policy能够产生对于这种偏差的鲁棒性。</p>
<h4 id="Policy-Architecture"><a href="#Policy-Architecture" class="headerlink" title="Policy Architecture"></a>Policy Architecture</h4><p>为了得到一个更好的exploration的策略，这篇文章将不同分辨率的两个egocentric Map添加到网络的输入中，这样的俯视图的输入能够在一个更宏观的层面上提供有用的信息。整个算法的流程图如Figure 1所示。整个网络的特征包括俩部分:<br>(1) 第一人称RGB图像的信息通过一个预训练好的ResNet-18来提取特征。<br>(2) 两种不同分辨率的俯视图也通过ResNet-18来提取特征<br>最后将这两个特征拼接起来得到当前状态的特征表达，然后通过RNN网络来形成最终的Policy。强化学习算法使用了imitation learning+PPO的方式。训练的时候首先利用Das et al.等人在House3D环境中所做的工作，直接将这个算法在环境中采集的轨迹作为专家轨迹来对网络进行训练。之后再采用PPO对算法进行优化。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/architecture.png"/></div>
<center><b>Figure 1. Architecture of algorithm</b></center>

<h4 id="Coverage-Reward"><a href="#Coverage-Reward" class="headerlink" title="Coverage Reward"></a>Coverage Reward</h4><p>为了更有效地使用环境俯视图提取出的特征，这篇工作设计了一个coverage reward。 $C(M_{t})$记录了整个环境中所有已知经过的区域和已知的未经过的区域，那么在智能体执行过一次动作$a_{t}$后，$C(M_{t+1})$和$C(M_{t})$会有所不同，那么exploration策略的目的应该是最大化已知经过区域的变化。因此Coverage Reward记为<br>$$ R_{cov}(t) = C(M_{t+1}) - C(M_{t}) $$<br>同时为了让智能体拥有避障的功能，还对碰撞行为进行了惩罚。碰撞的判定是通过一个额外的bump sensor来给出的，记为<br>$$ R_{coll}(t) = -Bump(t+1) $$<br>整个环境的reward为:<br>$$ r_{t} = R_{cov}+R_{coll} $$</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这篇工作使用了House3D仿真环境，针对训练和测试，分别使用了20个不同的House，共计40个House。测试使用的House和训练使用的House是完全不同的。这个场景中的observation包括了60°第一人称视角下的RGB-D相机信息和一个碰撞传感器。动作空间包括了6种基本动作，分别是：前进0.25米，后退0.25米，向左0.25米，向右0.25米，向左转9°和向右转9°。这个场景下没有任何的外部奖励，内在奖励记为$ R(t) = \alpha R_{cov}(t)+\beta R_{coll}(t) $。训练策略的时候，首先使用Das等人的方法采集693条human trajectories，然后使用这些轨迹进行imitation learning，形成一个最基础的policy。接下来再使用强化学习的方式形成一个更优秀的导航策略。House3D仿真环境如Figure 2所示。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/House3D.png"/></div>
<center><b>Figure 2. House3D screenshot</b></center>

<h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>这篇文章比较的Baselines主要包括两大类：(1) Froniter-based Exploration 这个方法每次都在建立好的环境地图中采样一个未被探索的目标位置，然后用路径规划的方式走到目标位置，随着这个流程不断进行，整个场景的地图可以建立的越来越完备。(2) Curiosity-based Exploration 这里作者采用了ICM类似的方式，使用prediction error作为intrinsic reward。</p>
<h3 id="Without-Estimation-Noise"><a href="#Without-Estimation-Noise" class="headerlink" title="Without Estimation Noise"></a>Without Estimation Noise</h3><p>这个场景的假设是位置估计$\hat{x_{t}}$是完全准确的，虽然这个假设非常的奢侈，但是可以比较一下不同探索方式的优劣性。这里作者比较了如下几种探索方式：Random Policy, IL+RL with Curiosity, Straight, IL+RL with RGB, IL+RL with Map+RGB。其中Straight的探索方式是指，智能体一直保持直行，如果碰撞，则转9°之后继续直行的探索方式。IL+RL with RGB是作者本文提出的方法，但是去掉了egocentric地图的输入，保留了coverage reward和其它部分。IL+RL with Map+RGB则是作者本文方法的完整版。这个实验的结果图如Figure 3所示，可以看出作者的方法是要优于简单的Curiosity-based Exploration的。并且单纯使用RGB作为输入的方法也比较优秀。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Methods.png"/></div>
<center><b>Figure 3. Exploration performance for different methods</b></center>

<h3 id="With-Estimation-Noise"><a href="#With-Estimation-Noise" class="headerlink" title="With Estimation Noise"></a>With Estimation Noise</h3><p>由于上一个实验的假设比较苛刻，因此作者对位置观测加入了噪声，并比较了噪声的比例对Coverage的影响，结果如Figure 4所示。可以看出作者的方法对于噪声带来的影响要比Froniter-based方式拥有更好的鲁棒性。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Noise.png"/></div>
<center><b>Figure 4. Exploration performance of different noise percentage</b></center>

<h3 id="Geometry-and-Affordance-Mismatch"><a href="#Geometry-and-Affordance-Mismatch" class="headerlink" title="Geometry and Affordance Mismatch"></a>Geometry and Affordance Mismatch</h3><p>作者用这个实验来说明learning的重要性。在这个实验中的设定是把House3D中的doors设置为不触发碰撞检测的障碍物，这个实验想要说明的是，当几何信息出现错误的时候，learning-based的方法拥有更强的鲁棒性，因为learning-based的方法能够从视觉上检测出门其实对应是一个障碍物从而弥补几何信息上的错误，这个实验的结果如Figure 5所示。可以看出Froniter-based方法对于几何信息的准确性较为敏感，因此在表现上有了大幅度的下降，而作者的方法并没有太多的表现下降。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Exploration-Mismatch.png"/></div>
<center><b>Figure 5. Exploration performance of mismatch for geometry</b></center>

<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>这篇文章中作者进行了三个消融实验分别验证Imitation Learning的重要性，环境地图作为输入的重要性以及coverage reward的优越性，实验结果如Figure 6所示。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Ablation-Study.png"/></div>
<center><b>Figure 6. Ablation Study</b></center>

<h3 id="Using-Exploration-For-Downstream-Task"><a href="#Using-Exploration-For-Downstream-Task" class="headerlink" title="Using Exploration For Downstream Task"></a>Using Exploration For Downstream Task</h3><p>进一步验证exploration的有效性，作者利用学到的exploration策略用于goal-driven navigation任务中去。goal的采集流程是在环境中随机选择50个goal image(随机位置，随机方向)。智能体需要利用exploration策略采集的得到的经验导航至目标图片。具体做法是对于目标所在的位置，首先使用Nearest Neighbor找到experience中距离最近的一张图片，然后从环境地图中找到对应位置，接下来使用路径规划的方式到达目标图片。因此在这个实验中，exploration策略能否采集到整个环境中不同区域的信息是非常重要的。作者比较了random policy,curiosity和作者的方法，结果如Figure 7所示。</p>
<div align=center><img src="https://raw.githubusercontent.com/Jthon-lab/image_repo/master/2020.11.9/Downstreamtask.png"/></div>
<center><b>Figure 7. Performance for goal-driven navigation</b></center>
            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Jthon lab. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar_duck.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jthon lab</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>RL</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Nanjing,China
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-spuaps2qplesxnme8e5wnd40hjkfzjjsqbnwhi8zohmoqbez3cm8pj5m7ejx.min.js"></script>

<!--SCRIPTS END-->





    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
